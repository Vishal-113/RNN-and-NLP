{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhdHKIiz3Mf7yx0qHgRRoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/RNN-and-NLP/blob/main/_Sentiment_Analysis_Q%26A_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the main architectural difference between BERT and GPT?\n",
        "Ans:\n",
        "The original Transformer has two main parts:\n",
        "\tEncoder: Processes the entire input all at once.\n",
        "\tDecoder: Generates output step by step (like in translation or text generation).\n",
        "* BERT uses only the encoder part of the Transformer architecture.\n",
        "* GPT uses only the decoder part of the Transformer.\n",
        "* BERT is bidirectional, meaning it looks at the entire sentence at once to understand context.\n",
        "* GPT is unidirectional (left-to-right), meaning it generates text one word at a time, based on previous words.\n",
        "* BERT is designed for understanding tasks like classification, NER, and QA.\n",
        "* GPT is designed for generative tasks like text completion, summarization, and dialogue.\n",
        "\n",
        "\n",
        "2. Why is using pre-trained models like BERT or GPT beneficial instead of training from scratch?\n",
        "Ans: \tPretrained models are trained on massive text corpora and learn general language features.\n",
        "\tThey save time and resources—you don’t have to train from scratch.\n",
        "\tThey require much less labeled data for your specific task (thanks to transfer learning).\n",
        "\tThey are more accurate, especially on small datasets, because they already understand grammar and language structure.\n",
        "\tUsing them helps avoid the need for powerful GPUs/TPUs and weeks of training time.\n",
        "\tThey are widely supported and regularly updated by the research community.\n"
      ],
      "metadata": {
        "id": "Pl6bqQd1yhSk"
      }
    }
  ]
}